{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "448c68cf-6187-467d-8c8a-044531936f7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Shared/Common Utilities/helper_utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7049f508-62f8-42f4-aa4c-3e4f115099de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the Data pretty printer - pprint library to help print data structures in\n",
    "# a readable, pretty way\n",
    "\n",
    "import pprint\n",
    "\n",
    "#  To utilize the configuration values in the helper_utils Notebook\n",
    "\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1880fed5-d280-483b-832f-aeed33326a94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_location = \"wasbs://sas-extracts@cctdash0000prd0504blob.blob.core.windows.net/Weekly .xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b9a86d-7cb7-447a-ab7d-978e7e7bd579",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write explanation here\n",
    "# table_name = temp_table_source = config[\"cct_db_temp_schema\"] + \".temp_test_delme\"\n",
    "\n",
    "#any point? \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "#comes from  helper_utils.py\n",
    "moh_hosp_weekly_icu_df = spark.read.format(\"com.crealytics.spark.excel\").options(header=True,inferSchema=True).load(file_location) \n",
    "\n",
    "\n",
    "moh_hosp_weekly_icu_df.createOrReplaceTempView(\"moh_hosp_weekly_icu_df\")\n",
    "\n",
    "query = \"SELECT * from moh_hosp_weekly_icu\"\n",
    "#display(spark.sql(query))\n",
    "    \n",
    "#create_table_as_query_and_assign_perms(\"weeklyICU_mohsas_df\", query) # fn coming from  helper_utils.py => /Shared/Common Utilities/helper_utils\n",
    "    \n",
    "#spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df90d72-2a06-4193-9c60-edad2478cd29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Week: string (nullable = true)\n",
      " |-- Available baseline beds (avg): double (nullable = true)\n",
      " |-- Adult baseline occupancy: double (nullable = true)\n",
      " |-- Vented adult baseline occupancy: double (nullable = true)\n",
      " |-- Adult baseline beds (avg): double (nullable = true)\n",
      " |-- Vented adult baseline beds (avg): double (nullable = true)\n",
      " |-- Adult baseline patients (avg): double (nullable = true)\n",
      " |-- Vented adult baseline patients (avg): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moh_hosp_weekly_icu_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1ac8aaa-5ffe-4bb6-9797-ac6951dcdf2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#renames columns\n",
    "\n",
    "moh_hosp_weekly_icu_df=make_col_names_safe(moh_hosp_weekly_icu_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a59475a-84b4-4b35-a5eb-520c96f5d1ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "moh_hosp_icu_weekly_table_source = config[\"cct_db_temp_schema\"] + \".temp_test\"\n",
    "print(moh_hosp_icu_weekly_table_source)\n",
    "spark.sql(f\"DROP TABLE IF EXISTS hive_metastore.default.moh_hosp_icu_weekly_table_source\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {temp_table_source}\") \n",
    "display(moh_hosp_weekly_icu_df)\n",
    "moh_hosp_weekly_icu_df.write.saveAsTable(\"moh_hosp_icu_weekly_table_source\")\n",
    "\n",
    "\n",
    " #hive_metastore.default.temp_table_source\n",
    "\n",
    "# #moh_hosp_weekly_icu_df = make_col_names_safe(moh_hosp_weekly_icu_df)\n",
    "# moh_hosp_weekly_icu_df.write.saveAsTable(temp_table_source)\n",
    "\n",
    "#moh_hosp_weekly_icu_df.withColumnRenamed(\"week\",\"reported_week\").write.saveAsTable(moh_hosp_icu_weekly_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5620d671-7674-479f-9642-843b2f22d1e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "development.public.public_test\n",
      "'development.public.public_test'"
     ]
    }
   ],
   "source": [
    "test_file = config[\"cct_db_schema\"] + \".public_test\"\n",
    "print(test_file)\n",
    "\n",
    "# prevent table duplication\n",
    "spark.sql(f\"DROP TABLE  IF EXISTS {test_file}\")\n",
    "# moh_hosp_weekly_icu_df.write.saveAsTable(test_file)\n",
    "display(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8685a33-a0ab-4b4d-aa89-bc6e4b1204df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "def second_dateFix(str):\n",
    "    lowered = str.lower().replace(\" \", '')\n",
    "\n",
    "    second_type_of_date = re.compile(\"[0-9]{4}[a-z]{3}[0-9]{6}\") #2020jan022021 (R)  #2020 Week20 May23 2021 (K)\n",
    "    match = second_type_of_date.match(lowered)\n",
    "    \n",
    "    if match is not None:\n",
    "        fp = re.sub(\"^[0-9]{4}\", '', lowered)\n",
    "        arr = re.findall('(^[a-z]{3})([0-9]{2})([0-9]{4})', fp)\n",
    "        return arr[0][2]+arr[0][0]+arr[0][1]\n",
    "        # return arr\n",
    "    else:\n",
    "        return str\n",
    "\n",
    "#lowered = str.lower().replace(\" \", '')\n",
    "\n",
    "\n",
    "def dateFix(str):\n",
    "    # lowered = str.lower().replace(\" \", '')\n",
    "\n",
    "    return re.sub(\"week[0-9][0-9]\", '', lowered)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7233463-635e-4d89-8668-18c0ce0f3d44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# moh_hosp_weekly_icu_df.select(\"week\").show()\n",
    "\n",
    "a = moh_hosp_weekly_icu_df.select(\"week\")\n",
    "# display(a)\n",
    "\n",
    "from pyspark.sql.functions import when, regexp_replace, col, concat, to_date\n",
    "# from pyspark.sql.functions import functions as f\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "b = a.withColumn('week', when(a.week.contains(\"Week\"),regexp_replace(a.week,'Week','')))\n",
    "b = b.withColumn('week', when(b.week.contains(\" \"),regexp_replace(b.week,' ','-')))\n",
    "split_col = f.split(b['week'], '-')\n",
    "\n",
    "b = b.withColumn('year', split_col.getItem(0))\\\n",
    "     .withColumn('week_num', split_col.getItem(1))\\\n",
    "     .withColumn('Month_date', split_col.getItem(2))\\\n",
    "     .withColumn('Year_latest', split_col.getItem(3))\n",
    "\n",
    "b = b.withColumn('Month', concat(b.Month_date.substr(0, 3)))\\\n",
    "     .withColumn('Date', concat(b.Month_date.substr(4, 5)))\n",
    "b = b.select('Date','Month','year',when(b.Year_latest.isNull(), b.year).otherwise(b.Year_latest).alias('Year_latest'))\n",
    "# display(b)\n",
    "\n",
    "b = b.withColumn('Date', f.concat(f.col('Date'), f.lit(\" \"), f.col('Month'), f.lit(\" \"), f.col('Year_latest')))\n",
    "b = b.drop(\"year\",\"week\",\"week_num\",\"Month_date\",\"Month\",\"Year_latest\")\n",
    "\n",
    "b = b.select(col(\"Date\"), to_date(col(\"Date\"), \"dd MMM yyyy\").alias(\"Transformed_Date\")).drop(\"Date\")\n",
    "display(b)  ######transformed date\n",
    "\n",
    "display(a)   ##original date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcec098e-a2db-411c-9093-b87d9b3fddb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "moh_hosp_weekly_icu_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df65eb4e-3464-407e-8c98-470bfdf6f94b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6495cf66-62e2-40ff-bc0a-84b5e0d3c147",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HospICUWeekly (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
